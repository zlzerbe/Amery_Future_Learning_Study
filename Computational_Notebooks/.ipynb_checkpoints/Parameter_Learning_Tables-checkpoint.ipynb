{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "388df6e4-4407-4617-9d61-211cfc40124f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling DataFramesMeta [1313f7d8-7da2-5740-9ea0-a2ca25f37964] \n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling UnitfulExt [8d0556db-720e-519a-baed-0b9ed79749be] (cache misses: wrong dep version loaded (2))\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling InterpolationsUnitfulExt [39de6958-b8ab-5ce0-9a8f-2bc2e9e79136] (cache misses: wrong dep version loaded (2))\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling InverseFunctionsUnitfulExt [f5f6e0dd-5310-5802-bcb2-1cb72ad693d4] (cache misses: wrong dep version loaded (2))\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling ConstructionBaseUnitfulExt [2b7dcb06-1a74-5ae7-a318-b62feea60883] (cache misses: wrong dep version loaded (2))\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "using Turing\n",
    "using LinearAlgebra\n",
    "using Distributions\n",
    "using MultivariateStats\n",
    "import MultivariateStats: reconstruct\n",
    "using GaussianProcesses\n",
    "using StatsBase\n",
    "using Statistics\n",
    "using Suppressor\n",
    "using JLD2\n",
    "using CSV\n",
    "using DataFrames, DataFramesMeta\n",
    "using SplitApplyCombine\n",
    "using KernelFunctions\n",
    "using MCMCChains\n",
    "using PyCall\n",
    "using PyPlot\n",
    "using Printf\n",
    "import PyCall.pyfunction\n",
    "using Missings\n",
    "\n",
    "os = pyimport(\"os\")\n",
    "pyimport(\"scienceplots\")\n",
    "np = pyimport(\"numpy\")\n",
    "\n",
    "\n",
    "PyCall.pygui(:tk)\n",
    "\n",
    "FONTSIZE=20.5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f1a91c-26b3-4068-89fe-9547faa350b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5aa5ac-0334-444f-a7c8-fdf2581842c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1325dfc8-0d6c-41c4-811f-6caaf14853bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "###############################                Parameter Learning Tables             ###########################\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################\n",
    "#####################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdae3ae1-b84e-4a31-94d9-59584d00c36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"../Data/Parameter_Learning_Tables/CI_width_variation_log_gamma0_true.csv\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_up_low(post_dict, upper_quant, lower_quant, γ0_is_log)\n",
    "    p = [upper_quant,lower_quant]\n",
    "    quantiles = zeros(19, 6, 2)\n",
    "    sorted_keys = sort(collect(keys(post_dict)))\n",
    "    # Adjust for when gamma_0 is considered on the log scale\n",
    "\n",
    "        for i in 1:19\n",
    "            mat = post_dict[ sorted_keys[i] ]\n",
    "            quantiles[i, 1,:] = quantile( mat[:,1], p )\n",
    "            quantiles[i, 2,:] = quantile( mat[:,2], p )\n",
    "            quantiles[i, 3,:] = quantile( mat[:,3], p )\n",
    "            quantiles[i, 4,:] = quantile( mat[:,4], p )\n",
    "            quantiles[i, 5,:] = (γ0_is_log) ? quantile( np.log(mat[:,5]), p ) : quantile( mat[:,5], p )\n",
    "            quantiles[i, 6,:] = quantile( mat[:,6], p )\n",
    "        end\n",
    "\n",
    "    return quantiles\n",
    "end\n",
    "\n",
    "Realizations = [string(i) for i in 1:100];\n",
    "cal_years = collect(range(2015,step=15,length=20));\n",
    "\n",
    "# Load the present day posteriors \n",
    "gamma_0_is_log = true\n",
    "present_posterior = np.load(\"../Data/Training_Data/posterior_samples_All_Combined.npy\");\n",
    "\n",
    "# Compute upper and lower quantile bounds for present day posteriors\n",
    "present_bounds = []\n",
    "present_widths = []\n",
    "for i in 1:6\n",
    "    lower = (i == 5 && gamma_0_is_log) ? quantile(np.log(present_posterior[:,i]), 0.05) : quantile(present_posterior[:,i], 0.05)\n",
    "    upper = (i == 5 && gamma_0_is_log) ? quantile(np.log(present_posterior[:,i]), 0.95) : quantile(present_posterior[:,i], 0.95)\n",
    "    push!(present_bounds, (lower,upper))\n",
    "    push!(present_widths, upper - lower)\n",
    "end\n",
    "present_bounds = reshape(present_bounds, 1, :)\n",
    "present_widths = reshape(present_widths, 1, :);\n",
    "\n",
    "\n",
    "# Change to wherever your posterior dictionaries are located\n",
    "path_to_posteriors = \"../Data/Posterior_Data\"\n",
    "\n",
    "# Initialize empty vectors to hold interval bounds and widths \n",
    "all_widths = zeros(length(Realizations),19,6)\n",
    "all_bounds = zeros(length(Realizations), 19, 6, 2)\n",
    "\n",
    "\n",
    "# Loop to calculate quantiles and credible interval widths\n",
    "    for (iter, r) in enumerate(Realizations)\n",
    "        post = JLD2.load(\"$(path_to_posteriors)/R_$(r)_Posterior_Dict.jld2\", \"post_data\")\n",
    "        cred_int_90_up_low = get_up_low(post, 0.95, 0.05,gamma_0_is_log)\n",
    "        all_bounds[iter, :, :, :] = cred_int_90_up_low\n",
    "        all_widths[iter,:,:] = cred_int_90_up_low[:,:,1] .- cred_int_90_up_low[:,:,2]\n",
    "    end\n",
    "# The average 5th percentile and and 95th percentiles of the six MALI parameters across all trajectories over the 19 years of calibration\n",
    "avg_bounds = mean(all_bounds,dims=1)\n",
    "avg_bounds = dropdims(avg_bounds, dims=1)\n",
    "\n",
    "# The average 90% credible interval widths of the six MALI parameters across all trajectories for each of the 19 years of calibration\n",
    "avg_widths = mean(all_widths,dims=1)\n",
    "avg_widths = dropdims(avg_widths, dims=1)\n",
    "println(size(avg_widths))\n",
    "# The 17% and 83% quantiles of the 90% cred int widths of the six MALI parameters across all trajectories for each of the 19 years of calibration\n",
    "width_variation_interval = np.quantile(all_widths, 0.83,axis=0) .- np.quantile(all_widths, 0.17,axis=0)\n",
    "\n",
    "\n",
    "# Store the bounds as tuples in one matrix\n",
    "bounds_as_tuples = fill((0.0,0.0),(19,6))\n",
    "for i in 1:19\n",
    "    for j in 1:6\n",
    "        bounds_as_tuples[i,j] = (avg_bounds[i,j,2], avg_bounds[i,j,1])\n",
    "    end\n",
    "end\n",
    "# Add the 2015 90% credible interval bounds\n",
    "bounds_w_present = vcat(present_bounds, bounds_as_tuples);\n",
    "\n",
    "# Add the 2015 90% credible widths\n",
    "widths_w_present = vcat(present_widths, avg_widths);\n",
    "\n",
    "\n",
    "# Add the years of calibration as another column\n",
    "bounds_w_years = hcat(cal_years, bounds_w_present)\n",
    "widths_w_years = hcat(cal_years, widths_w_present)\n",
    "width_var_w_years = hcat(cal_years[2:end], width_variation_interval)\n",
    "\n",
    "# Dataframe column names\n",
    "names = [\"Latest_calibration_year\",\"vmThresh\",\"fricExp\",\"mu_scale\",\"stiff_scale\",\"gamma0\",\"melt_flux\"]\n",
    "\n",
    "# Covnert to datframe before saving to disc\n",
    "params_df_bounds = DataFrame(bounds_w_years, names)\n",
    "params_df_widths = DataFrame(widths_w_years, names)\n",
    "params_df_width_var = DataFrame(width_var_w_years, names)\n",
    "\n",
    "#Calculate total percent decrease of 90% cred interval width\n",
    "pct_change = ((collect(params_df_widths[1,2:end]) .- collect(params_df_widths[20,2:end])) ./ collect(params_df_widths[1,2:end])) .* 100\n",
    "# Make a 1×7 row: first element is label, then pct_change values\n",
    "new_row = hcat(\"total_%_decrease\", pct_change...)\n",
    "# Convert to DataFrame with the same column names\n",
    "new_df = DataFrame(new_row, names)\n",
    "# Append to params_df_widths\n",
    "append!(params_df_widths, new_df)\n",
    "\n",
    "\n",
    "CSV.write(\"../Data/Parameter_Learning_Tables/CI_bounds_log_gamma0_$(gamma_0_is_log).csv\",params_df_bounds)\n",
    "CSV.write(\"../Data/Parameter_Learning_Tables/CI_widths_log_gamma0_$(gamma_0_is_log).csv\",params_df_widths)\n",
    "CSV.write(\"../Data/Parameter_Learning_Tables/CI_width_variation_log_gamma0_$(gamma_0_is_log).csv\",   params_df_width_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f30841-4e55-47df-a0a5-03609ccc5678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"../Data/Parameter_Learning_Tables/yr_to_yr_change_in_widths_log_gamma0_true.csv\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##CALCULATING THE YEAR BY YEAR CHANGE IN UNCERTAINTY RANGES FOR EACH PARAMETER, AS WELL AS PERCENT CONTRIBUTION TO TOTAL LEARNING\n",
    "##\n",
    "cal_years = collect(range(2015,step=15,length=20))\n",
    "\n",
    "Δ_param_width = zeros(20, 6)\n",
    "Δ_param_width_pct = zeros(20, 6)\n",
    "\n",
    "#Total learning from 2015 - 2300\n",
    "\n",
    "# find row indices\n",
    "total_idx1 = only(findall(x -> x == 2015, cal_years))\n",
    "total_idx2 = only(findall(x -> x == 2300, cal_years))\n",
    "\n",
    "# extract vectors (dropping year column)\n",
    "present = collect(params_df_widths[total_idx1, :])[2:end]\n",
    "final = collect(params_df_widths[total_idx2, :])[2:end]\n",
    "\n",
    "# compute differences\n",
    "total_diff = final .- present\n",
    "Δ_param_width[20, :] = total_diff\n",
    "Δ_param_width_pct[20,:] .= 100.0\n",
    "\n",
    "learn_periods = []\n",
    "\n",
    "for i in 1:(length(cal_years)-1)\n",
    "    yr1 = cal_years[i]\n",
    "    yr2 = cal_years[i+1]\n",
    "\n",
    "    # find row indices\n",
    "    row_idx1 = only(findall(x -> x == yr1, cal_years))\n",
    "    row_idx2 = only(findall(x -> x == yr2, cal_years))\n",
    "\n",
    "    # extract vectors (dropping year column)\n",
    "    row1 = collect(params_df_widths[row_idx1, :])[2:end]\n",
    "    row2 = collect(params_df_widths[row_idx2, :])[2:end]\n",
    "\n",
    "    # compute differences\n",
    "    diff = row2 .- row1\n",
    "    Δ_param_width[i, :] = diff\n",
    "    Δ_param_width_pct[i, :] = (diff ./ total_diff) .* 100\n",
    "    # Collect this learning period as string for datframe labels\n",
    "    push!(learn_periods, \"$(yr1) - $(yr2)\")\n",
    "end\n",
    "\n",
    "# Add the total learning period\n",
    "push!(learn_periods, \"2015 - 2300\")\n",
    "width_change_mat = zeros(20,12)\n",
    "\n",
    "# Put the parameter width changes and percent changes together param by param columnwise\n",
    "for i in 1:6\n",
    "    width_change_mat[:,(2*i)-1] = Δ_param_width[:,i]\n",
    "    width_change_mat[:,(2*i)] = Δ_param_width_pct[:,i]\n",
    "end\n",
    "\n",
    "\n",
    "names = [\"LearningPeriod\",\"ΔvmThresh\",\"Δ_%_vmThresh\",\"Δ_fricExp\",\"Δ_%_fricExp\",\"Δ_mu_scale\",\"Δ_%_mu_scale\",\"Δ_stiff_scale\",\"Δ_%_stiff_scale\",\"Δ_gamma0\",\"Δ_%_gamma0\",\"Δ_melt_flux\",\"Δ_%_melt_flux\"]\n",
    "width_change_w_years = hcat(learn_periods, width_change_mat)\n",
    "df_bounds_yr_to_yr = DataFrame(width_change_w_years, names)\n",
    "CSV.write(\"../Data/Parameter_Learning_Tables/yr_to_yr_change_in_widths_log_gamma0_$(gamma_0_is_log).csv\", df_bounds_yr_to_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b4034d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00000000000001\n"
     ]
    }
   ],
   "source": [
    "println(sum(df_bounds_yr_to_yr[1:5, 11]) + sum(df_bounds_yr_to_yr[6:13, 11]) + sum(df_bounds_yr_to_yr[14:19, 11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e662d9ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Δ_%_vmThresh decrease from 2015 to 2090: \n",
      "11.193044442419605\n",
      "Δ_%_fricExp decrease from 2015 to 2090: \n",
      "13.56602081348913\n",
      "Δ_%_mu_scale decrease from 2015 to 2090: \n",
      "24.11285593408966\n",
      "Δ_%_stiff_scale decrease from 2015 to 2090: \n",
      "25.69691638711342\n",
      "Δ_%_gamma0 decrease from 2015 to 2090: \n",
      "58.146813524449946\n",
      "Δ_%_melt_flux decrease from 2015 to 2090: \n",
      "30.490298144376688\n"
     ]
    }
   ],
   "source": [
    "cols = [3,5,7,9,11,13]\n",
    "for (i,col) in enumerate(cols)\n",
    "    println(\"$(names[2i+1]) decrease from 2015 to 2090: \")\n",
    "    println(sum(df_bounds_yr_to_yr[1:5, col]))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46313e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Δ_%_vmThresh decrease from 2090 to 2210: \n",
      "65.35858638372444\n",
      "Δ_%_fricExp decrease from 2090 to 2210: \n",
      "56.43968759374293\n",
      "Δ_%_mu_scale decrease from 2090 to 2210: \n",
      "65.15053906260987\n",
      "Δ_%_stiff_scale decrease from 2090 to 2210: \n",
      "61.183643628975034\n",
      "Δ_%_gamma0 decrease from 2090 to 2210: \n",
      "36.681516511449466\n",
      "Δ_%_melt_flux decrease from 2090 to 2210: \n",
      "56.939614177713494\n"
     ]
    }
   ],
   "source": [
    "cols = [3,5,7,9,11,13]\n",
    "for (i,col) in enumerate(cols)\n",
    "    println(\"$(names[2i+1]) decrease from 2090 to 2210: \")\n",
    "    println(sum(df_bounds_yr_to_yr[6:13, col]))\n",
    "end\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec208a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Δ_%_vmThresh decrease from 2210 to 2300: \n",
      "23.448369173855948\n",
      "Δ_%_fricExp decrease from 2210 to 2300: \n",
      "29.99429159276793\n",
      "Δ_%_mu_scale decrease from 2210 to 2300: \n",
      "10.736605003300483\n",
      "Δ_%_stiff_scale decrease from 2210 to 2300: \n",
      "13.119439983911551\n",
      "Δ_%_gamma0 decrease from 2210 to 2300: \n",
      "5.171669964100604\n",
      "Δ_%_melt_flux decrease from 2210 to 2300: \n",
      "12.570087677909815\n"
     ]
    }
   ],
   "source": [
    "cols = [3,5,7,9,11,13]\n",
    "for (i,col) in enumerate(cols)\n",
    "    println(\"$(names[2i+1]) decrease from 2210 to 2300: \")\n",
    "    println(sum(df_bounds_yr_to_yr[14:19, col]))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f9e18f4-875e-46a2-bc3e-318afd6d173a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"../Data/Parameter_Learning_Tables/yr_to_yr_change_in_width_variation_log_gamma0_true.csv\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##CALCULATING THE YEAR BY YEAR CHANGE IN UNCERTAINTY RANGE STANDARD DEVIATION FOR EACH PARAMETER, AS WELL AS PERCENT CONTRIBUTION TO TOTAL CHANGE\n",
    "##\n",
    "cal_years = collect(range(2030,step=15,length=19))\n",
    "\n",
    "Δ_width_var = zeros(19, 6)\n",
    "Δ_width_var_pct = zeros(19, 6)\n",
    "\n",
    "#Total learning from 2015 - 2300\n",
    "\n",
    "# find row indices\n",
    "total_idx1 = only(findall(x -> x == 2030, cal_years))\n",
    "total_idx2 = only(findall(x -> x == 2300, cal_years))\n",
    "\n",
    "# extract vectors (dropping year column)\n",
    "present = collect(params_df_width_var[total_idx1, :])[2:end]\n",
    "final = collect(params_df_width_var[total_idx2, :])[2:end]\n",
    "\n",
    "# compute differences\n",
    "total_diff = final .- present\n",
    "Δ_width_var[19, :] = round.(total_diff, sigdigits = 6) #20\n",
    "Δ_width_var_pct[19,:] .= 100.0\n",
    "\n",
    "learn_periods = []\n",
    "\n",
    "for i in 1:(length(cal_years)-1)\n",
    "    yr1 = cal_years[i]\n",
    "    yr2 = cal_years[i+1]\n",
    "\n",
    "    # find row indices\n",
    "    row_idx1 = only(findall(x -> x == yr1, cal_years))\n",
    "    row_idx2 = only(findall(x -> x == yr2, cal_years))\n",
    "\n",
    "    # extract vectors (dropping year column)\n",
    "    row1 = collect(params_df_width_var[row_idx1, :])[2:end]\n",
    "    row2 = collect(params_df_width_var[row_idx2, :])[2:end]\n",
    "\n",
    "    # compute differences\n",
    "    diff = row2 .- row1\n",
    "    Δ_width_var[i, :] = round.(diff, sigdigits = 6)\n",
    "    Δ_width_var_pct[i, :] = (diff ./ total_diff) .* 100\n",
    "    # Collect this learning period as string for datframe labels\n",
    "    push!(learn_periods, \"$(yr1) - $(yr2)\")\n",
    "end\n",
    "# Add the total learning period (2015 not included since multiple trajectories only began with 2030 caliobration onward)\n",
    "push!(learn_periods, \"2030 - 2300\")\n",
    "width_variance_change = zeros(19,12)\n",
    "\n",
    "# Put the parameter width changes and percent changes together param by param columnwise\n",
    "for i in 1:6\n",
    "    width_variance_change[:,(2*i)-1] = Δ_width_var[:,i]\n",
    "    width_variance_change[:,(2*i)] = Δ_width_var_pct[:,i]\n",
    "end\n",
    "\n",
    "\n",
    "names = [\"LearningPeriod\",\"ΔvmThresh\",\"Δ_%_vmThresh\",\"Δ_fricExp\",\"Δ_%_fricExp\",\"Δ_mu_scale\",\"Δ_%_mu_scale\",\"Δ_stiff_scale\",\"Δ_%_stiff_scale\",\"Δ_gamma0\",\"Δ_%_gamma0\",\"Δ_melt_flux\",\"Δ_%_melt_flux\"]\n",
    "width_variance_w_years = hcat(learn_periods, width_variance_change)\n",
    "df_width_variance_yr_to_yr = DataFrame(width_variance_w_years , names)\n",
    "CSV.write(\"../Data/Parameter_Learning_Tables/yr_to_yr_change_in_width_variation_log_gamma0_$(gamma_0_is_log).csv\", df_width_variance_yr_to_yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3197284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89973969-e1ea-471e-a2cb-fd6eba0124cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b1acfad-e0db-4463-8c32-540907062547",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fae2a70-cca9-4b8a-90a3-86eecba0eefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19,)\n",
      "(19,)\n",
      "(19,)\n"
     ]
    }
   ],
   "source": [
    "# The years of projection considered\n",
    "chosen_projection_years = [2100,2200,2300]\n",
    "\n",
    "# Initialize empty arrays (Some entries will be missing due to calibration past year of projection)\n",
    "all_bounds   = Array{Array{Union{Missing,Tuple{Missing, Missing},Tuple{Float64,Float64},Int64}}}(undef, length(chosen_projection_years))\n",
    "all_widths   = zeros(3,20)\n",
    "all_width_variation = zeros(3,19)\n",
    "# For each projection year considered\n",
    "for (n, year) in enumerate(chosen_projection_years)\n",
    "    \n",
    "    proj_yr_idx = yrs_dict[year]\n",
    "    proj_bounds = Array{Union{Missing,Float64}}(undef,length(Realizations),19,2)\n",
    "    proj_widths = zeros(100,19)\n",
    "    \n",
    "    # For each of the 100 trajectories considered\n",
    "    for j in 1:length(Realizations)\n",
    "        # For each of the 19 calibrating years considered\n",
    "        for (iter,yr) in enumerate(cal_years[2:end])\n",
    "                cal_year_matrix = JLD2.load(\"../Data/Projection_Data/R_$(Realizations[j])/$(Realizations[j])-year$(yr)pred_VAF.jld2\",\n",
    "                    \"sample_post_mm_ssp5\")\n",
    "                proj_bounds[j,iter,1] = quantile(cal_year_matrix[proj_yr_idx,:], 0.05)\n",
    "                proj_bounds[j,iter,2] = quantile(cal_year_matrix[proj_yr_idx,:], 0.95)\n",
    "                proj_widths[j,iter] = proj_bounds[j,iter,2] - proj_bounds[j,iter,1]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Average across the 100 trajectories\n",
    "    avg_bounds = dropdims(mean(proj_bounds,dims=1),dims=1)\n",
    "    avg_widths = dropdims(mean(proj_widths,dims=1),dims=1)\n",
    "\n",
    "    # Compute 66% credible interval of the 90% interval widths across the 100 trajectories\n",
    "    width_variation_interval = np.quantile(proj_widths, 0.83,axis=0) .- np.quantile(proj_widths,0.17,axis=0) #31\n",
    "    println(size(width_variation_interval))\n",
    "\n",
    "\n",
    "    # Load the present day projections\n",
    "    present_projections = JLD2.load(\"../Data/Projection_Data/2015_SLR_Projections.jld2\",\"present2015_post_mm_ssp5\")\n",
    "    present_lower = quantile(present_projections[proj_yr_idx,:], 0.05)\n",
    "    present_upper = quantile(present_projections[proj_yr_idx,:], 0.95)\n",
    "    present_width = present_upper - present_lower\n",
    "\n",
    "    # Add present day projections to beginning of lists above\n",
    "    bounds_w_present = vcat([present_lower;; present_upper], avg_bounds)\n",
    "    widths_w_present = vcat([present_width], avg_widths);\n",
    "    # Store the bounds as tuples in one matrix\n",
    "    slr_tuple_bounds = [(round(bounds_w_present[i,1],sigdigits=6),round(bounds_w_present[i,2],sigdigits=6)) for i in 1:length(bounds_w_present[:,1])]\n",
    "\n",
    "    # Push this projection years quantities to list containing all the projection years\n",
    "    all_bounds[n]   = slr_tuple_bounds\n",
    "    all_widths[n,:]   =  widths_w_present\n",
    "    all_width_variation[n,:]  =  width_variation_interval\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1684fda1-7917-49d7-a273-29374a7e84db",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_years = collect(range(2015,step=15,length=20));\n",
    "cal_years = Int.(cal_years)\n",
    "\n",
    "# Add the years of calibration as another column\n",
    "bounds_w_years = hcat(cal_years, reduce(hcat, all_bounds))\n",
    "widths_w_years = hcat(cal_years, all_widths')\n",
    "width_var_w_years = hcat(cal_years[2:end],all_width_variation' )\n",
    "\n",
    "names = [\"Latest year of calibration\",\"2100\", \"2200\",\"2300\"]\n",
    "\n",
    "# Convert to dataframes before saving to disc\n",
    "slr_df_bounds  = DataFrame(bounds_w_years, names)\n",
    "slr_df_widths  = DataFrame(widths_w_years, names)\n",
    "slr_df_width_var = DataFrame(width_var_w_years, names)\n",
    "allowmissing!(slr_df_bounds) \n",
    "allowmissing!(slr_df_widths) \n",
    "allowmissing!(slr_df_width_var);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c66738f-366b-42f2-95c1-b663f78db914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix{Any}\n",
      "\u001b[1m21×4 DataFrame\u001b[0m\n",
      "\u001b[1m Row \u001b[0m│\u001b[1m Latest year of calibration \u001b[0m\u001b[1m 2100    \u001b[0m\u001b[1m 2200    \u001b[0m\u001b[1m 2300    \u001b[0m\n",
      "     │\u001b[90m Any                        \u001b[0m\u001b[90m Any     \u001b[0m\u001b[90m Any     \u001b[0m\u001b[90m Any     \u001b[0m\n",
      "─────┼───────────────────────────────────────────────────────\n",
      "   1 │ 2015.0                      5.81829  65.2261  76.6676\n",
      "   2 │ 2030.0                      4.62033  52.8798  70.4075\n",
      "   3 │ 2045.0                      4.42315  40.4086  69.8543\n",
      "   4 │ 2060.0                      4.22244  22.92    67.1312\n",
      "   5 │ 2075.0                      4.14338  21.8803  66.1392\n",
      "   6 │ 2090.0                      4.02504  20.5155  61.188\n",
      "   7 │ 2105.0                      0.0      17.3835  58.2574\n",
      "   8 │ 2120.0                      0.0      15.5806  56.4196\n",
      "   9 │ 2135.0                      0.0      14.8113  53.0541\n",
      "  10 │ 2150.0                      0.0      14.4658  50.1841\n",
      "  11 │ 2165.0                      0.0      13.6753  42.5954\n",
      "  12 │ 2180.0                      0.0      12.23    35.005\n",
      "  13 │ 2195.0                      0.0      10.0191  29.0676\n",
      "  14 │ 2210.0                      0.0      0.0      20.4525\n",
      "  15 │ 2225.0                      0.0      0.0      17.0559\n",
      "  16 │ 2240.0                      0.0      0.0      15.0905\n",
      "  17 │ 2255.0                      0.0      0.0      14.1069\n",
      "  18 │ 2270.0                      0.0      0.0      13.3744\n",
      "  19 │ 2285.0                      0.0      0.0      12.9264\n",
      "  20 │ 2300.0                      0.0      0.0      0.0\n",
      "  21 │ total_%_decrease            30.8209  84.6394  83.1397\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"../Data/SLR_Learning_Tables/SLR_90_CI_vaXXXXXriation.csv\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove data that has been calibrated past its projection year (i.e projections for year 2100 with parameters calibrated in 2105)\n",
    "for yr in names[2:3]\n",
    "    n_useful = (parse(Int, yr) - 2015) ÷ 15\n",
    "    slr_df_bounds[n_useful+2:end, yr] .= missing\n",
    "    slr_df_widths[n_useful+2:end, yr] .= 0\n",
    "    slr_df_width_var[n_useful+1:end, yr] .= 0    \n",
    "\n",
    "end\n",
    "\n",
    "#Calculate total percent decrease of 90% cred interval width\n",
    "pct_change2100 = ((slr_df_widths[1,2] - slr_df_widths[6,2]) /  slr_df_widths[1,2]) * 100\n",
    "pct_change2200 = ((slr_df_widths[1,3] - slr_df_widths[13,3]) /  slr_df_widths[1,3]) * 100\n",
    "pct_change2300 = ((slr_df_widths[1,4] - slr_df_widths[19,4]) /  slr_df_widths[1,4]) * 100\n",
    "\n",
    "# Make a 1×7 row: first element is label, then pct_change values\n",
    "new_row = hcat(\"total_%_decrease\", pct_change2100, pct_change2200, pct_change2300)\n",
    "println(typeof(new_row))\n",
    "# Convert to DataFrame with the same column names\n",
    "new_df = DataFrame(new_row, names)\n",
    "\n",
    "\n",
    "# Append to params_df_widths\n",
    "append!(slr_df_widths, new_df, promote=true) #22\n",
    "println(slr_df_widths)\n",
    "\n",
    "CSV.write(\"../Data/SLR_Learning_Tables/SLR_90_CI_bounds.csv\", slr_df_bounds)\n",
    "CSV.write(\"../Data/SLR_Learning_Tables/SLR_90_CI_widths.csv\", slr_df_widths)\n",
    "CSV.write(\"../Data/SLR_Learning_Tables/SLR_90_CI_variation.csv\",  slr_df_width_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9f417d8-f020-48d2-9676-df4a9b34b8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((2300 - 2015) ÷ 15) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "977839e4-d682-4011-be67-80752bbd2a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATING THE YEAR TO YEAR CHANGE IN 90% UNCERTAINTY RANGES FOR PROJECTIONS OF THE YEARS 2100, 2200, 2300\n",
    "proj_years = [2100,2200,2300]\n",
    "cal_years = collect(range(2015, step=15, length=20))\n",
    "\n",
    "Δ_proj_widths_yty = Array{Array{Union{Float64, Missing}}}(undef, 6)\n",
    "for (i,yr) in enumerate(proj_years)\n",
    "    n_cal_years = ((yr - 2015) ÷ 15) + 1\n",
    "    # Calculate total learning\n",
    "    present_row_idx = only(findall(x -> x == 2015, cal_years))\n",
    "    present_diff = (i == 3) ? slr_df_widths[n_cal_years - 1, i+1] - slr_df_widths[present_row_idx,i+1] : slr_df_widths[n_cal_years, i+1] - slr_df_widths[present_row_idx,i+1]\n",
    "    abs_change = Array{Union{Float64, Missing}}(undef, 20)\n",
    "    pct_change = Array{Union{Float64, Missing}}(undef, 20)\n",
    "    # Calculate the year to year change and percent of total change\n",
    "    proj_yr_widths = slr_df_widths[!,i+1]\n",
    "    abs_change[1:19] = proj_yr_widths[2:20] - proj_yr_widths[1:19] #15\n",
    "    pct_change[1:19] = (abs_change[1:19] ./ present_diff) .* 100\n",
    "    # Add the total change to the end of the list\n",
    "    abs_change[20] = present_diff\n",
    "    # Concatenate to the full matrix\n",
    "    Δ_proj_widths_yty[2*i-1] = abs_change\n",
    "    Δ_proj_widths_yty[2*i] = pct_change\n",
    "        \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cd3c056-1c95-4dc7-b536-63b3c1af5787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"../Data/SLR_Learning_Tables/yr_to_yr_90_CI_widthXCXXXX_change.csv\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels for datframe rows\n",
    "learn_periods = vcat([\"$(cal_years[i]) - $(cal_years[i+1])\" for i in 1:(length(cal_years) - 1)], [\"Total learning\"])\n",
    "# Collect labels with data\n",
    "mat_yty_width_change = hcat(learn_periods, reduce(hcat, Δ_proj_widths_yty))\n",
    "# We don't care about the change in uncertainty past the point of projection\n",
    "mat_yty_width_change[6,2] = 0.0\n",
    "mat_yty_width_change[6,3] = 0.0\n",
    "mat_yty_width_change[13,4] = 0.0\n",
    "mat_yty_width_change[13,5] = 0.0\n",
    "mat_yty_width_change[19,6] = 0.0\n",
    "mat_yty_width_change[19,7] = 0.0\n",
    "# Column labels\n",
    "names = [\"Learning_Period\",\"Δ_2100\",\"Δ_%_2100\",\"Δ_2200\",\"Δ_%_2200\", \"Δ_2300\",\"Δ_%_2300\"]\n",
    "df_yty_width_change  = DataFrame(mat_yty_width_change, names)\n",
    "CSV.write(\"../Data/SLR_Learning_Tables/yr_to_yr_90_CI_width_change.csv\",   df_yty_width_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a4564d5-ba16-4e5d-a455-76f8f1ac4ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div style = \"float: left;\"><span>20×7 DataFrame</span></div><div style = \"clear: both;\"></div></div><div class = \"data-frame\" style = \"overflow-x: scroll;\"><table class = \"data-frame\" style = \"margin-bottom: 6px;\"><thead><tr class = \"header\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">Row</th><th style = \"text-align: left;\">Learning_Period</th><th style = \"text-align: left;\">Δ_2100</th><th style = \"text-align: left;\">Δ_%_2100</th><th style = \"text-align: left;\">Δ_2200</th><th style = \"text-align: left;\">Δ_%_2200</th><th style = \"text-align: left;\">Δ_2300</th><th style = \"text-align: left;\">Δ_%_2300</th></tr><tr class = \"subheader headerLastRow\"><th class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\"></th><th title = \"Any\" style = \"text-align: left;\">Any</th><th title = \"Any\" style = \"text-align: left;\">Any</th><th title = \"Any\" style = \"text-align: left;\">Any</th><th title = \"Any\" style = \"text-align: left;\">Any</th><th title = \"Any\" style = \"text-align: left;\">Any</th><th title = \"Any\" style = \"text-align: left;\">Any</th><th title = \"Any\" style = \"text-align: left;\">Any</th></tr></thead><tbody><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">1</td><td style = \"text-align: left;\">2015 - 2030</td><td style = \"text-align: left;\">-1.19796</td><td style = \"text-align: left;\">66.8038</td><td style = \"text-align: left;\">-12.3463</td><td style = \"text-align: left;\">22.3636</td><td style = \"text-align: left;\">-6.26004</td><td style = \"text-align: left;\">9.82103</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">2</td><td style = \"text-align: left;\">2030 - 2045</td><td style = \"text-align: left;\">-0.197182</td><td style = \"text-align: left;\">10.9958</td><td style = \"text-align: left;\">-12.4713</td><td style = \"text-align: left;\">22.59</td><td style = \"text-align: left;\">-0.553257</td><td style = \"text-align: left;\">0.867974</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">3</td><td style = \"text-align: left;\">2045 - 2060</td><td style = \"text-align: left;\">-0.200712</td><td style = \"text-align: left;\">11.1927</td><td style = \"text-align: left;\">-17.4885</td><td style = \"text-align: left;\">31.6781</td><td style = \"text-align: left;\">-2.7231</td><td style = \"text-align: left;\">4.27212</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">4</td><td style = \"text-align: left;\">2060 - 2075</td><td style = \"text-align: left;\">-0.0790525</td><td style = \"text-align: left;\">4.40833</td><td style = \"text-align: left;\">-1.0397</td><td style = \"text-align: left;\">1.88327</td><td style = \"text-align: left;\">-0.991968</td><td style = \"text-align: left;\">1.55624</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">5</td><td style = \"text-align: left;\">2075 - 2090</td><td style = \"text-align: left;\">-0.118345</td><td style = \"text-align: left;\">6.59948</td><td style = \"text-align: left;\">-1.3648</td><td style = \"text-align: left;\">2.47214</td><td style = \"text-align: left;\">-4.95117</td><td style = \"text-align: left;\">7.76762</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">6</td><td style = \"text-align: left;\">2090 - 2105</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-3.132</td><td style = \"text-align: left;\">5.67319</td><td style = \"text-align: left;\">-2.93064</td><td style = \"text-align: left;\">4.59772</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">7</td><td style = \"text-align: left;\">2105 - 2120</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">-1.80292</td><td style = \"text-align: left;\">3.26575</td><td style = \"text-align: left;\">-1.83781</td><td style = \"text-align: left;\">2.88323</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">8</td><td style = \"text-align: left;\">2120 - 2135</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">-0.769318</td><td style = \"text-align: left;\">1.39352</td><td style = \"text-align: left;\">-3.3655</td><td style = \"text-align: left;\">5.27994</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">9</td><td style = \"text-align: left;\">2135 - 2150</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">-0.345495</td><td style = \"text-align: left;\">0.625817</td><td style = \"text-align: left;\">-2.86996</td><td style = \"text-align: left;\">4.50251</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">10</td><td style = \"text-align: left;\">2150 - 2165</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">-0.790551</td><td style = \"text-align: left;\">1.43198</td><td style = \"text-align: left;\">-7.58876</td><td style = \"text-align: left;\">11.9056</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">11</td><td style = \"text-align: left;\">2165 - 2180</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">-1.44523</td><td style = \"text-align: left;\">2.61784</td><td style = \"text-align: left;\">-7.59036</td><td style = \"text-align: left;\">11.9081</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">12</td><td style = \"text-align: left;\">2180 - 2195</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">-2.2109</td><td style = \"text-align: left;\">4.00475</td><td style = \"text-align: left;\">-5.93743</td><td style = \"text-align: left;\">9.3149</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">13</td><td style = \"text-align: left;\">2195 - 2210</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-8.61511</td><td style = \"text-align: left;\">13.5158</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">14</td><td style = \"text-align: left;\">2210 - 2225</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">-3.3966</td><td style = \"text-align: left;\">5.32874</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">15</td><td style = \"text-align: left;\">2225 - 2240</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">-1.96539</td><td style = \"text-align: left;\">3.08338</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">16</td><td style = \"text-align: left;\">2240 - 2255</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">-0.983606</td><td style = \"text-align: left;\">1.54312</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">17</td><td style = \"text-align: left;\">2255 - 2270</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">-0.732502</td><td style = \"text-align: left;\">1.14918</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">18</td><td style = \"text-align: left;\">2270 - 2285</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">-0.447993</td><td style = \"text-align: left;\">0.702831</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">19</td><td style = \"text-align: left;\">2285 - 2300</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">-0.0</td><td style = \"text-align: left;\">0.0</td><td style = \"text-align: left;\">0.0</td></tr><tr><td class = \"rowNumber\" style = \"font-weight: bold; text-align: right;\">20</td><td style = \"text-align: left;\">Total learning</td><td style = \"text-align: left;\">-1.79325</td><td style = \"font-style: italic; text-align: left;\">missing</td><td style = \"text-align: left;\">-55.207</td><td style = \"font-style: italic; text-align: left;\">missing</td><td style = \"text-align: left;\">-63.7412</td><td style = \"font-style: italic; text-align: left;\">missing</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccccc}\n",
       "\t& Learning\\_Period & Δ\\_2100 & Δ\\_\\%\\_2100 & Δ\\_2200 & Δ\\_\\%\\_2200 & Δ\\_2300 & Δ\\_\\%\\_2300\\\\\n",
       "\t\\hline\n",
       "\t& Any & Any & Any & Any & Any & Any & Any\\\\\n",
       "\t\\hline\n",
       "\t1 & 2015 - 2030 & -1.19796 & 66.8038 & -12.3463 & 22.3636 & -6.26004 & 9.82103 \\\\\n",
       "\t2 & 2030 - 2045 & -0.197182 & 10.9958 & -12.4713 & 22.59 & -0.553257 & 0.867974 \\\\\n",
       "\t3 & 2045 - 2060 & -0.200712 & 11.1927 & -17.4885 & 31.6781 & -2.7231 & 4.27212 \\\\\n",
       "\t4 & 2060 - 2075 & -0.0790525 & 4.40833 & -1.0397 & 1.88327 & -0.991968 & 1.55624 \\\\\n",
       "\t5 & 2075 - 2090 & -0.118345 & 6.59948 & -1.3648 & 2.47214 & -4.95117 & 7.76762 \\\\\n",
       "\t6 & 2090 - 2105 & 0.0 & 0.0 & -3.132 & 5.67319 & -2.93064 & 4.59772 \\\\\n",
       "\t7 & 2105 - 2120 & 0.0 & -0.0 & -1.80292 & 3.26575 & -1.83781 & 2.88323 \\\\\n",
       "\t8 & 2120 - 2135 & 0.0 & -0.0 & -0.769318 & 1.39352 & -3.3655 & 5.27994 \\\\\n",
       "\t9 & 2135 - 2150 & 0.0 & -0.0 & -0.345495 & 0.625817 & -2.86996 & 4.50251 \\\\\n",
       "\t10 & 2150 - 2165 & 0.0 & -0.0 & -0.790551 & 1.43198 & -7.58876 & 11.9056 \\\\\n",
       "\t11 & 2165 - 2180 & 0.0 & -0.0 & -1.44523 & 2.61784 & -7.59036 & 11.9081 \\\\\n",
       "\t12 & 2180 - 2195 & 0.0 & -0.0 & -2.2109 & 4.00475 & -5.93743 & 9.3149 \\\\\n",
       "\t13 & 2195 - 2210 & 0.0 & -0.0 & 0.0 & 0.0 & -8.61511 & 13.5158 \\\\\n",
       "\t14 & 2210 - 2225 & 0.0 & -0.0 & 0.0 & -0.0 & -3.3966 & 5.32874 \\\\\n",
       "\t15 & 2225 - 2240 & 0.0 & -0.0 & 0.0 & -0.0 & -1.96539 & 3.08338 \\\\\n",
       "\t16 & 2240 - 2255 & 0.0 & -0.0 & 0.0 & -0.0 & -0.983606 & 1.54312 \\\\\n",
       "\t17 & 2255 - 2270 & 0.0 & -0.0 & 0.0 & -0.0 & -0.732502 & 1.14918 \\\\\n",
       "\t18 & 2270 - 2285 & 0.0 & -0.0 & 0.0 & -0.0 & -0.447993 & 0.702831 \\\\\n",
       "\t19 & 2285 - 2300 & 0.0 & -0.0 & 0.0 & -0.0 & 0.0 & 0.0 \\\\\n",
       "\t20 & Total learning & -1.79325 & \\emph{missing} & -55.207 & \\emph{missing} & -63.7412 & \\emph{missing} \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m20×7 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m Learning_Period \u001b[0m\u001b[1m Δ_2100     \u001b[0m\u001b[1m Δ_%_2100 \u001b[0m\u001b[1m Δ_2200    \u001b[0m\u001b[1m Δ_%_2200 \u001b[0m\u001b[1m Δ_2300    \u001b[0m\u001b[1m \u001b[0m ⋯\n",
       "     │\u001b[90m Any             \u001b[0m\u001b[90m Any        \u001b[0m\u001b[90m Any      \u001b[0m\u001b[90m Any       \u001b[0m\u001b[90m Any      \u001b[0m\u001b[90m Any       \u001b[0m\u001b[90m \u001b[0m ⋯\n",
       "─────┼──────────────────────────────────────────────────────────────────────────\n",
       "   1 │ 2015 - 2030      -1.19796    66.8038   -12.3463   22.3636   -6.26004    ⋯\n",
       "   2 │ 2030 - 2045      -0.197182   10.9958   -12.4713   22.59     -0.553257\n",
       "   3 │ 2045 - 2060      -0.200712   11.1927   -17.4885   31.6781   -2.7231\n",
       "   4 │ 2060 - 2075      -0.0790525  4.40833   -1.0397    1.88327   -0.991968\n",
       "   5 │ 2075 - 2090      -0.118345   6.59948   -1.3648    2.47214   -4.95117    ⋯\n",
       "   6 │ 2090 - 2105      0.0         0.0       -3.132     5.67319   -2.93064\n",
       "   7 │ 2105 - 2120      0.0         -0.0      -1.80292   3.26575   -1.83781\n",
       "   8 │ 2120 - 2135      0.0         -0.0      -0.769318  1.39352   -3.3655\n",
       "   9 │ 2135 - 2150      0.0         -0.0      -0.345495  0.625817  -2.86996    ⋯\n",
       "  10 │ 2150 - 2165      0.0         -0.0      -0.790551  1.43198   -7.58876\n",
       "  11 │ 2165 - 2180      0.0         -0.0      -1.44523   2.61784   -7.59036\n",
       "  12 │ 2180 - 2195      0.0         -0.0      -2.2109    4.00475   -5.93743\n",
       "  13 │ 2195 - 2210      0.0         -0.0      0.0        0.0       -8.61511    ⋯\n",
       "  14 │ 2210 - 2225      0.0         -0.0      0.0        -0.0      -3.3966\n",
       "  15 │ 2225 - 2240      0.0         -0.0      0.0        -0.0      -1.96539\n",
       "  16 │ 2240 - 2255      0.0         -0.0      0.0        -0.0      -0.983606\n",
       "  17 │ 2255 - 2270      0.0         -0.0      0.0        -0.0      -0.732502   ⋯\n",
       "  18 │ 2270 - 2285      0.0         -0.0      0.0        -0.0      -0.447993\n",
       "  19 │ 2285 - 2300      0.0         -0.0      0.0        -0.0      0.0\n",
       "  20 │ Total learning   -1.79325   \u001b[90m missing  \u001b[0m -55.207   \u001b[90m missing  \u001b[0m -63.7412  \u001b[90m \u001b[0m\n",
       "\u001b[36m                                                                1 column omitted\u001b[0m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_yty_width_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ce7d079-e5f8-4ec5-b55a-6daa3b4f958b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.51500443127654\n"
     ]
    }
   ],
   "source": [
    "println(sum(df_yty_width_change[1:4, 5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a826b97-9fe6-4d21-987a-698be6308a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATING THE YEAR TO YEAR CHANGE IN 90% UNCERTAINTY RANGE STANDARD DEVIATION FOR PROJECTIONS OF THE YEARS 2100, 2200, 2300\n",
    "proj_years = [2100,2200,2300]\n",
    "cal_years = collect(range(2030, step=15, length=19))\n",
    "\n",
    "Δ_proj_width_var_yty = Array{Array{Union{Float64, Missing}}}(undef, 6)\n",
    "for (i,yr) in enumerate(proj_years)\n",
    "    n_cal_years = ((yr - 2030) ÷ 15) + 1\n",
    "    # Calculate total learning\n",
    "    present_row_idx = only(findall(x -> x == 2030, cal_years))\n",
    "    present_diff = (i == 3) ? slr_df_width_var[n_cal_years - 1, i+1] - slr_df_width_var[present_row_idx,i+1] : slr_df_width_var[n_cal_years, i+1] - slr_df_width_var[present_row_idx,i+1]\n",
    "    abs_change = Array{Union{Float64, Missing}}(undef, 19)\n",
    "    pct_change = Array{Union{Float64, Missing}}(undef, 19)\n",
    "    # Calculate the year to year change and percent of total change\n",
    "    proj_yr_width_var = slr_df_width_var[!,i+1]\n",
    "    abs_change[1:18] = proj_yr_width_var[2:19] - proj_yr_width_var[1:18]\n",
    "    pct_change[1:18] = (abs_change[1:18] ./ present_diff) .* 100\n",
    "    # Add the total change to the end of the list\n",
    "    abs_change[19] = present_diff\n",
    "    # Concatenate to the full matrix\n",
    "    Δ_proj_width_var_yty[2*i-1] = abs_change\n",
    "    Δ_proj_width_var_yty[2*i] = pct_change\n",
    "        \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56a81fba-c407-4517-817b-99e1a54c515b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"../Data/SLR_Learning_Tables/yr_to_yr_90_CI_widthXXXX_variation_change.csv\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels for datframe rows\n",
    "learn_periods = vcat([\"$(cal_years[i]) - $(cal_years[i+1])\" for i in 1:(length(cal_years) - 1)], [\"Total learning\"])\n",
    "# Collect labels with data\n",
    "mat_yty_std_change = hcat(learn_periods, reduce(hcat, Δ_proj_width_var_yty))\n",
    "# We don't care about the change in uncertainty variation past the point of projection\n",
    "mat_yty_std_change[5,2] = 0.0\n",
    "mat_yty_std_change[5,3] = 0.0\n",
    "mat_yty_std_change[12,4] = 0.0\n",
    "mat_yty_std_change[12,5] = 0.0\n",
    "mat_yty_std_change[18,6] = 0.0\n",
    "mat_yty_std_change[18,7] = 0.0\n",
    "# Column labels\n",
    "names = [\"Learning_Period\",\"Δ_2100\",\"Δ_%_2100\",\"Δ_2200\",\"Δ_%_2200\", \"Δ_2300\",\"Δ_%_2300\"]\n",
    "df_yty_std_change  = DataFrame(mat_yty_std_change, names)\n",
    "CSV.write(\"../Data/SLR_Learning_Tables/yr_to_yr_90_CI_width_variation_change.csv\",   df_yty_std_change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da7b02-27cf-4b79-8477-308ace2a1114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b83a504-6364-47d7-9709-711f219f6233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

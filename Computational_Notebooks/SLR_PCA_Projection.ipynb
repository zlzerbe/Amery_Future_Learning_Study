{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fcc615f-0e47-4f64-99a3-d0b2eb4edee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "# import Pkg\n",
    "# Pkg.add(\"QuantileRegressions\")\n",
    "using Turing\n",
    "using LinearAlgebra\n",
    "using Distributions\n",
    "using MultivariateStats\n",
    "import MultivariateStats: reconstruct\n",
    "using GaussianProcesses\n",
    "using Optim\n",
    "using StatsBase\n",
    "using Statistics\n",
    "using Suppressor\n",
    "using JLD2\n",
    "using CSV\n",
    "using DataFrames, DataFramesMeta\n",
    "using SplitApplyCombine\n",
    "using KernelFunctions\n",
    "using MCMCChains\n",
    "using QuantileRegressions\n",
    "using PyCall\n",
    "using PyPlot\n",
    "using Printf\n",
    "import PyCall.pyfunction\n",
    "\n",
    "# Access the matplotlib module\n",
    "matplotlib = pyimport(\"matplotlib\")\n",
    "mpl_inset_locator = pyimport(\"mpl_toolkits.axes_grid1.inset_locator\")\n",
    "seaborn = pyimport(\"seaborn\")\n",
    "pyimport(\"scienceplots\")\n",
    "scipy = pyimport(\"scipy\")\n",
    "np = pyimport(\"numpy\")\n",
    "skl_model_selection = pyimport(\"sklearn.model_selection\")\n",
    "seaborn.color_palette(\"colorblind\")\n",
    "\n",
    "PyCall.pygui(:tk)\n",
    "\n",
    "\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "using Random\n",
    "Random.seed!(11);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a964d628-4dc3-4b02-8a3a-149695f60128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reconstruct (generic function with 7 methods)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project(pca, Y, J=size(Y,1)) = (projection(pca)[:,1:J])' * centralize(Y, pca.mean)\n",
    "reconstruct(pca, W, J=size(W,1)) = decentralize(projection(pca)[:,1:J] * W, mean(pca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11056cc5-f122-453c-9df9-de169dcde5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = CSV.read(\"../Data/Training_Data/Amery_VAF_time_series_2300_ssp_5_8.5.csv\", DataFrame)\n",
    "y = select(y, Not(1))\n",
    "y = Matrix(y)\n",
    "y = collect(y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f670e16c-9ae4-4fd0-b76f-50f31d4f1318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of the data matrix: 119\n"
     ]
    }
   ],
   "source": [
    "rank_data = rank(y)\n",
    "println(\"Rank of the data matrix: \", rank_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ec4d39d-03a8-45d8-8131-1e14661111a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = fit(PCA, y, pratio=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "226151d6-5b4b-438d-bdc7-3cdf06ac571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 5 # Change this to the number of PCs you want\n",
    "pred_pre = project(p, y, K)\n",
    "rec_pre = reconstruct(p, pred_pre, K);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ccc38ca-62a4-4fc3-9289-f40a85800246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Plots\n",
    "# i = sample(1:size(y,2))\n",
    "# Plots.plot(y[:,i], lw=2, title=\"i = $i\")\n",
    "# Plots.plot!(rec_pre[:,i], lw=2, line=:dash, color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cefb3faf-b886-4630-8ec5-061c771bcf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Plots\n",
    "# # i = sample(1:size(y,2))\n",
    "# plt = Plots.plot()\n",
    "# for i = 1:size(y,2)\n",
    "#     Plots.plot!(y[:,i], lw=2, color=:lightskyblue, legend = false)\n",
    "#     Plots.plot!(rec_pre[:,i], lw=2, line=:dash, color=\"red\", legend = false)\n",
    "# end\n",
    "# # plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef084aa0-c81b-46a3-b812-ec4e10ec488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = CSV.read(\"../Data/Training_Data/Amery_GP_Emulator_RELX_Ensemble_Filtered.csv\", DataFrame);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bf50629c-5dcb-4b5f-8b7b-1b2943b5ee02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6×119 adjoint(::Matrix{Float64}) with eltype Float64:\n",
       " 0.407891   0.584329  0.879853  0.35452   …  0.89069    0.435987   0.727361\n",
       " 0.0356264  0.509296  0.437061  0.789839     0.0620034  0.412618   0.840317\n",
       " 0.354441   0.449709  0.200242  0.311622     0.394692   0.0504651  0.712968\n",
       " 0.485306   0.770992  0.664396  0.733916     0.555406   0.342915   0.709687\n",
       " 0.607262   0.45634   0.784528  0.921009     0.959423   0.688045   0.451809\n",
       " 0.91086    0.239113  0.835963  0.482451  …  0.11399    0.180736   0.683243"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmThresh_min, vmThresh_max = minimum(x.vmThresh), maximum(x.vmThresh)\n",
    "scaler_vmThresh = fit(UnitRangeTransform, x.vmThresh)\n",
    "d_01 = StatsBase.transform(scaler_vmThresh, x.vmThresh)\n",
    "\n",
    "fricExp_min, fricExp_max = minimum(x.fricExp), maximum(x.fricExp)\n",
    "scaler_fricExp = fit(UnitRangeTransform, x.fricExp)\n",
    "d_02 = StatsBase.transform(scaler_fricExp, x.fricExp)\n",
    "\n",
    "mu_scale_min, mu_scale_max = minimum(x.mu_scale), maximum(x.mu_scale)\n",
    "scaler_mu_scale = fit(UnitRangeTransform, x.mu_scale)\n",
    "d_03 = StatsBase.transform(scaler_mu_scale, x.mu_scale)\n",
    "\n",
    "stiff_scale_min, stiff_scale_max = minimum(x.stiff_scale), maximum(x.stiff_scale)\n",
    "scaler_stiff_scale = fit(UnitRangeTransform, x.stiff_scale)\n",
    "d_04 = StatsBase.transform(scaler_stiff_scale, x.stiff_scale)\n",
    "\n",
    "gamma0_min, gamma0_max = minimum(x.gamma0), maximum(x.gamma0)\n",
    "scaler_gamma0 = fit(UnitRangeTransform, x.gamma0)\n",
    "d_05 = StatsBase.transform(scaler_gamma0, x.gamma0)\n",
    "\n",
    "melt_flux_min, melt_flux_max = minimum(x.melt_flux), maximum(x.melt_flux)\n",
    "scaler_melt_flux = fit(UnitRangeTransform, x.melt_flux)\n",
    "d_06 = StatsBase.transform(scaler_melt_flux, x.melt_flux)\n",
    "\n",
    "param_mins = [vmThresh_min,fricExp_min,mu_scale_min,stiff_scale_min,gamma0_min,melt_flux_min]\n",
    "param_maxs = [vmThresh_max,fricExp_max,mu_scale_max,stiff_scale_max,gamma0_max,melt_flux_max]\n",
    "\n",
    "design_scaled = DataFrame(vmThresh_scaled=d_01, fricExp_scaled=d_02, mu_scale_scaled=d_03,\n",
    "                          stiff_scale_scaled=d_04, gamma0_scaled=d_05, melt_flux_scaled=d_06)\n",
    "\n",
    "X = Matrix(design_scaled)' # matrix 6 x N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28eca75e-4309-4211-bbf1-257f82b517bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "function scale_pcs(pc_matrix, num_pcs)\n",
    "    pc_scaled = Vector{Vector{Float64}}(undef, num_pcs)\n",
    "    scalers = Vector{UnitRangeTransform}(undef, num_pcs)\n",
    "    pc_min = Vector{Float64}(undef, num_pcs)\n",
    "    pc_max = Vector{Float64}(undef, num_pcs)\n",
    "    \n",
    "    for i in 1:num_pcs\n",
    "        pc_values = pc_matrix[i, :]\n",
    "        pc_min[i], pc_max[i] = minimum(pc_values), maximum(pc_values)\n",
    "        scaler = fit(UnitRangeTransform, pc_values)\n",
    "        scalers[i] = scaler\n",
    "        pc_scaled[i] = StatsBase.transform(scaler, pc_values)\n",
    "    end\n",
    "    \n",
    "    return pc_scaled, scalers, pc_min, pc_max\n",
    "end\n",
    "\n",
    "# Create a 2D array where each row represents a different PC\n",
    "# You can change the number of rows (i.e., the number of PCs) dynamically\n",
    "PC_scaled, scalers, PC_min, PC_max = scale_pcs(pred_pre, K);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96fc6642-2f47-4000-a5cf-a1f9e4105fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39moptimize! did not converge on attempt 1 with 100 iterations.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Main In[32]:23\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mRetrying with 200 iterations...\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSuccessfully trained emulator in 200 iterations.\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSuccessfully trained emulator in 100 iterations.\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSuccessfully trained emulator in 100 iterations.\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39moptimize! did not converge on attempt 1 with 100 iterations.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Main In[32]:23\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mRetrying with 200 iterations...\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSuccessfully trained emulator in 200 iterations.\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39moptimize! did not converge on attempt 1 with 100 iterations.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Main In[32]:23\u001b[39m\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mRetrying with 200 iterations...\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mSuccessfully trained emulator in 200 iterations.\n"
     ]
    }
   ],
   "source": [
    "function fit_gp(X_train, y_train)\n",
    "    mZero = MeanLin(zeros(size(X_train, 1)))  # Mean function \n",
    "    ## GaussianProcesses.MeanZero() # Zero mean function\n",
    "    kern = Mat52Ard(zeros(size(X_train, 1)), 0.001)  # Kernel function \n",
    "    ##+ GaussianProcesses.Noise(0.0) # GaussianProcesses.SE(0.0,0.001)\n",
    "    logObsNoise = -4.605170185985  # Log observation noise\n",
    "\n",
    "    gp = GaussianProcesses.GP(X_train, y_train, mZero, kern, logObsNoise)\n",
    "    \n",
    "#     optimize!(gp, domean = true, kern = true, noise = false, iterations = 100)\n",
    "    iters = 100\n",
    "    max_retries = 5\n",
    "    \n",
    "    for attempt in 0:5\n",
    "            \n",
    "        res = optimize!(gp; domean=true, kern=true, noise=false, iterations=iters)\n",
    "\n",
    "        if Optim.converged(res)\n",
    "            success = true\n",
    "            break\n",
    "        else\n",
    "            success = false\n",
    "            @warn \"optimize! did not converge on attempt $(attempt+1) with $iters iterations.\"\n",
    "            if attempt < max_retries\n",
    "                iters *= 2\n",
    "                @info \"Retrying with $iters iterations...\"\n",
    "                gp = GaussianProcesses.GP(X_train, y_train, mZero, kern, logObsNoise)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    @info \"Successfully trained emulator in $iters iterations.\"\n",
    "    \n",
    "    return gp\n",
    "end\n",
    "\n",
    "function predict_gp_eml(gp, X_test)\n",
    "    μ, var = GaussianProcesses.predict(gp, X_test)\n",
    "    return μ, sqrt.(var)\n",
    "end\n",
    "\n",
    "gps = [fit_gp(X, pc_scaled) for pc_scaled in PC_scaled];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f52744c3-75ed-4860-9bd0-4d0be06b5295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_pcs = K\n",
    "\n",
    "#  Save the relevant variables into disk:\n",
    "#@save \"VAF_PCA_GP_Emulators.jld2\" gps scalers PC_min PC_max p num_pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb6316cf-0275-4b21-ba26-e48a214705e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 6)\n",
      "{Float64}\n"
     ]
    }
   ],
   "source": [
    "# RELX calibrated posterior\n",
    "θ_posterior_expert = np.load(\"../Data/Training_Data/posterior_samples_All_Combined.npy\")\n",
    "println(size(θ_posterior_expert))\n",
    "println(typeof(θ_posterior_expert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "728b5775-3032-458c-954b-94da4f9e8d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert priors\n",
    "post_full_2015 = DataFrame(vmThresh_post=θ_posterior_expert[:,1], fricExp_post=θ_posterior_expert[:,2], \n",
    "    mu_scale_post=θ_posterior_expert[:,3], stiff_scale_post=θ_posterior_expert[:,4], \n",
    "    gamma0_post=θ_posterior_expert[:,5], melt_flux_post=θ_posterior_expert[:,6]);\n",
    "\n",
    "Random.seed!(11)\n",
    "\n",
    "sample_size = 5000\n",
    "\n",
    "# Get the total number of rows in the DataFrame\n",
    "total_rows = nrow(post_full_2015)\n",
    "# Generate random indices to select rows\n",
    "random_indices = randperm(total_rows)[1:sample_size]\n",
    "# Select the subset of rows using the random indices\n",
    "post_2015 = post_full_2015[random_indices, :]\n",
    "\n",
    "\n",
    "\n",
    "vmThresh_prior = scipy.stats.truncnorm.rvs(a=-3, b=3, loc=130000, scale=50000/3, size=sample_size)\n",
    "fricExp_prior = scipy.stats.trapezoid.rvs(c=0.05/0.233, d=0.18/0.233, loc=0.1, scale=0.233, size=sample_size)\n",
    "mu_scale_prior = scipy.stats.truncnorm.rvs(a=-2, b=2, loc=1.0, scale=0.1, size=sample_size)\n",
    "stiff_scale_prior = scipy.stats.truncnorm.rvs(a=-2, b=2, loc=1.0, scale=0.1, size=sample_size)\n",
    "trunc_norm_gamma0 = scipy.stats.truncnorm(a=np.log(9620)-10, b=np.log(471000)-10, loc=10, scale=1)\n",
    "gamma0_prior = np.exp(trunc_norm_gamma0.rvs(size=sample_size))\n",
    "melt_flux_prior = scipy.stats.truncnorm.rvs(a=-2, b=2, loc=35, scale=11.5, size=sample_size)\n",
    "\n",
    "prior = DataFrame(vmThresh_prior=vmThresh_prior, fricExp_prior=fricExp_prior, mu_scale_prior=mu_scale_prior, \n",
    "    stiff_scale_prior=stiff_scale_prior, gamma0_prior=gamma0_prior, melt_flux_prior=melt_flux_prior);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73702a66-4146-4d6f-a315-d5a4f19f8843",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pcs = size(PC_scaled,1)\n",
    "mu_post = [Vector{Float64}() for _ in 1:num_pcs]\n",
    "sig_post = [Vector{Float64}() for _ in 1:num_pcs]\n",
    "\n",
    "for x in eachrow(post_2015)\n",
    "    p1_t = (x.vmThresh_post-vmThresh_min)/(vmThresh_max-vmThresh_min)\n",
    "    p2_t = (x.fricExp_post-fricExp_min)/(fricExp_max-fricExp_min)\n",
    "    p3_t = (x.mu_scale_post-mu_scale_min)/(mu_scale_max-mu_scale_min)\n",
    "    p4_t = (x.stiff_scale_post-stiff_scale_min)/(stiff_scale_max-stiff_scale_min)\n",
    "    p5_t = (x.gamma0_post-gamma0_min)/(gamma0_max-gamma0_min)\n",
    "    p6_t = (x.melt_flux_post-melt_flux_min)/(melt_flux_max-melt_flux_min)\n",
    "    \n",
    "    α = [p1_t;; p2_t;; p3_t;; p4_t;; p5_t;; p6_t]'\n",
    "    \n",
    "    for i in 1:num_pcs\n",
    "        μ, σ = predict_gp_eml(gps[i], α)\n",
    "        mu =  only(μ)*(PC_max[i] - PC_min[i]) + PC_min[i]\n",
    "        sig = only(σ)*(PC_max[i] - PC_min[i])\n",
    "        append!(mu_post[i], mu)\n",
    "        append!(sig_post[i], sig)\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31a6f159-354f-4d36-8f55-87ae3d097cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_mean_pred_post = hcat(mu_post...)\n",
    "PC_sig_pred_post = hcat(sig_post...)\n",
    "\n",
    "sample_post = Matrix{Float64}(undef, 0, size(y, 1))\n",
    "\n",
    "for (mu_sample, sigma_sample) in zip(eachrow(PC_mean_pred_post), eachrow(PC_sig_pred_post))\n",
    "    # Appendix B.2 steps combined to generate multivariate normal back tranformed sample\n",
    "    sample = reconstruct(p, rand.(Normal.(mu_sample,sigma_sample)), num_pcs)\n",
    "    sample_post = [sample_post; sample']\n",
    "end\n",
    "sample_post = sample_post'\n",
    "    \n",
    "#Convert to Sea level contribution\n",
    "present2015_post_mm_ssp5 = sample_post./(-362);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b9bd40f-950d-451e-9275-584713f0676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Code below for projecting forward with sequences of future calibrated posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2dfd502c-cc9f-4530-98e3-cce6c6e04830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential_prediction_func (generic function with 1 method)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function that projects sea level contribution for the sequence of calibrated posteriors 2030-2300 for a given realization(trajectory)\n",
    "function sequential_prediction_func(\n",
    "        posterior_dict::Dict{Int64, Matrix{Float64}},\n",
    "        realization::String,\n",
    "        num_pcs,\n",
    "        min_PC,\n",
    "        max_PC,\n",
    "        X_mins,\n",
    "        X_maxs,\n",
    "        fit,\n",
    "        vfa,\n",
    "        emulators\n",
    "\n",
    "    )\n",
    "        \n",
    "        sample_size = 5000\n",
    "        projection_dict = Dict{Int64, Matrix{Float64}}()\n",
    "        #Project forward for each posterior in dictionary\n",
    "        \n",
    "        for key in keys(posterior_dict)\n",
    "            #println(key)\n",
    "            post_full = DataFrame(vmThresh_post=posterior_dict[key][:,1], fricExp_post=posterior_dict[key][:,2], \n",
    "            mu_scale_post=posterior_dict[key][:,3], stiff_scale_post=posterior_dict[key][:,4], \n",
    "            gamma0_post=posterior_dict[key][:,5], melt_flux_post=posterior_dict[key][:,6])\n",
    "\n",
    "            Random.seed!(11)\n",
    "            # Get the total number of rows in the DataFrame\n",
    "            total_rows = nrow(post_full)\n",
    "            # Generate random indices to select rows\n",
    "            random_indices = randperm(total_rows)[1:sample_size]\n",
    "            # Select the subset of rows using the random indices\n",
    "            post = post_full[random_indices, :]\n",
    "\n",
    "            #Initialize empty arrays for saving\n",
    "            mu_post = [Vector{Float64}() for _ in 1:num_pcs]\n",
    "            sig_post = [Vector{Float64}() for _ in 1:num_pcs]\n",
    "            \n",
    "            #Predict with each PC emulator for samples of the posterior\n",
    "            for x in eachrow(post)\n",
    "                #scale parameters\n",
    "                p1_t = (x.vmThresh_post- X_mins[1])/(X_maxs[1]-X_mins[1])\n",
    "                p2_t = (x.fricExp_post- X_mins[2])/(X_maxs[2]-X_mins[2])\n",
    "                p3_t = (x.mu_scale_post- X_mins[3])/(X_maxs[3]-X_mins[3])\n",
    "                p4_t = (x.stiff_scale_post - X_mins[4])/(X_maxs[4]-X_mins[4])\n",
    "                p5_t = (x.gamma0_post- X_mins[5])/(X_maxs[5]-X_mins[5])\n",
    "                p6_t = (x.melt_flux_post- X_mins[6])/(X_maxs[6]-X_mins[6])\n",
    "                \n",
    "                α = [p1_t;; p2_t;; p3_t;; p4_t;; p5_t;; p6_t]'\n",
    "                \n",
    "                #Predict on each PC\n",
    "                for i in 1:num_pcs\n",
    "                    μ, σ = predict_gp_eml(emulators[i], α)\n",
    "                    mu =  only(μ)*(max_PC[i] - min_PC[i]) + min_PC[i]\n",
    "                    sig = only(σ)*(max_PC[i] - min_PC[i])\n",
    "                    append!(mu_post[i], mu)\n",
    "                    append!(sig_post[i], sig)\n",
    "                end\n",
    "            end\n",
    "\n",
    "            #Save the Principal components predictions \n",
    "            PC_mean_pred_post = hcat(mu_post...)\n",
    "            PC_sig_pred_post = hcat(sig_post...)\n",
    "\n",
    "            # mkpath(\"../../SLR_Projection_Data/pred_mean_PC/R_$(realization)\")\n",
    "            # mkpath(\"../../SLR_Projection_Data/pred_std_PC/R_$(realization)\")\n",
    "            # @save \"../../SLR_Projection_Data/pred_mean_PC/R_$(realization)/$(realization)-year$(key)mean_PCs.jld2\" PC_mean_pred_post\n",
    "            # @save \"../../SLR_Projection_Data/pred_std_PC/R_$(realization)/$(realization)-year$(key)std_PCs.jld2\" PC_sig_pred_post\n",
    "\n",
    "            sample_post = Matrix{Float64}(undef, 0, size(vfa, 1))\n",
    "\n",
    "            #Reconstruct the PC predictions to get VAF outputs\n",
    "            for (mu_sample, sigma_sample) in zip(eachrow(PC_mean_pred_post), eachrow(PC_sig_pred_post))\n",
    "                # Appendix B.2 steps combined to generate multivariate normal back tranformed sample\n",
    "                sample = reconstruct(fit, rand.(Normal.(mu_sample,sigma_sample)), num_pcs)\n",
    "                sample_post = [sample_post; sample']\n",
    "            end\n",
    "            sample_post = sample_post'\n",
    "            sample_post_mm_ssp5 = sample_post./(-362)\n",
    "            projection_dict[key] = sample_post_mm_ssp5\n",
    "            mkpath(\"../Data/ProjectionData/R_$(realization)\")\n",
    "            @save \"../Data/ProjectionData/R_$(realization)/$(realization)-year$(key)pred_VAF.jld2\" sample_post_mm_ssp5\n",
    "         \n",
    "\n",
    "        end\n",
    "    \n",
    "        return nothing\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4869c-d1e6-473e-a544-e0d436067e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell to project foward for a set of trajectories of posteriors\n",
    "\n",
    "#Realization numbers\n",
    "Realizations = [i for i in 1:100];\n",
    "#Misconverged realization numbers\n",
    "m = [4, 15, 20, 44, 47, 67, 81, 82, 89, 100]\n",
    "filter!(x -> !(x in m), Realizations)\n",
    "r_filtered = [string(r) for r in Realizations];\n",
    "\n",
    "yrs_all = collect(range(2030, step=15, length=19));\n",
    "\n",
    "for r in realizations\n",
    "    θ_posterior_fut = JLD2.load(\"../Data/Posterior_Data/Ninety_Posterior_Sets/R_$(r)_Posterior_Dict.jld2\", \"post_data\");\n",
    "    sequential_prediction_func(\n",
    "        θ_posterior_fut,\n",
    "        r,\n",
    "        5,\n",
    "        PC_min,\n",
    "        PC_max,\n",
    "        param_mins,\n",
    "        param_maxs,\n",
    "        p,\n",
    "        y,\n",
    "        gps)\n",
    "    println(\"Done with realization $(r)...\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242f8d8a-01ba-4a25-a6d7-0e9ac1f7f407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
